#  数据业务

## 秒杀下的防止超卖

特征:秒杀商品库存总量固定 , 先到先得，瞬间并发极大，但写库量有限.

利用`redis实现计数器 与 decr原子操作` +` nginx 与 lua脚本` 完成.

![image-20211210205559293](picture/image-20211210205559293.png)

![image-20211210205955691](picture/image-20211210205955691.png)

主要关键点:

1. 利用预减库存方式杜绝超卖

2. 利用Nginx+Lua+Redis在网关层面将无效请求阻挡

3. 利用MQ消息队列的QoS限流特性保证MySQL不会被瞬间击垮

4. APP需要额外设计轮询机制查询订单状态(类似使用银行卡转账后的转圈圈)

>TIPS: 订单创建后，用户取消订单或未支付怎么办？
>
>为订单设置过期时间，订单支付有效期过期或用户取消订单 . LUA执行 incr “stk-1001” 自增即可







## 高并发下乐观锁解决并发数据冲突

> 悲观锁性能差,用户体验差 ,且现在主流场景如电商都是读多写少

通过给表添加version字段实现, 每次修改前先查询出版本,修改数据时先判断版本并版本加一.

如果遇到冲突怎么办?

1. 前端应用提示"数据正在处理 , 请稍后再试";
2. 附加spring-retry在service上进行方法重试

```
@Transactional
//重试异常  最大重试次数  重试间隔默认1S
@Retryable(value = {VersionException.class}, maxAttempts = 3)
public void updateBal(){
    Account acc =  执行：”select id,bal,_version from acc where id = 1001”;
    acc.setBal(acc.getBal() + 400)；
    int count = 执行：“update acc set bal = ${acc.bal} , _version=_version + 1 
		where id = 1001 and _version=${acc.version}”；
    if(count == 0) { throw new VersionException(“产生并发异常”) }；
}

```

> 额外问题: 为什么不直接一条语句update即可?   尽量不要将计算放在sql语句中



## 使用代理主键而不是业务主键

为什么表的主键要使用代理主键（自增编号, 无意义），不建议使用业务主键(如身份证)?

常规上来说

1. 业务主键更浪费空间

2. 业务主键无顺序，可能会造成写入数据时需要更长组织索引

3. 业务主键如果是字符串，在分库分表时，无法直接取模运算，需要先转换为数字，处理更麻烦了。

业务架构上来说:

1. 有时难免会遇到需要修改业务主键的情况, 而如果分布式下多个模块依赖到此主键, 则带来了很麻烦的连锁反应
2. 对于目前常使用的逻辑删除情况下,  如果要再次插入已逻辑删除的数据 , 而因为使用了逻辑主键, 出现了主键冲突 .

> 这里为什么不直接修改已逻辑删除的数据 , 因为大多数业务不允许将删除前后数据关联起来 ,并且要存留删除后的信息 , 如员工在职信息

> 这里为什么不对逻辑删除字段和身份证信息进行联合主键呢? 抛开增加了主键复杂度不说 ,  假如员工需要二次离职呢? 又会出现主键冲突.  
>
> 关于逻辑删除与唯一索引的冲突 , [可见唯一索引与逻辑删除的冲突](#逻辑删除与唯一索引) 

## 逻辑删除与唯一索引

**逻辑删除:**实际业务中, 对删除的数据给它设置一个标记"已删除", 在后续正常查询中,会过滤这条数据. 

有利于排查问题与补救.

通常设置标记的字段数据类型有布尔类型(mysql中可以使用tinyint) ; 字符串类型(y/n).

而逻辑删除与唯一索引共同使用时, 比如博客系统中,标题+作者是唯一的, 用户发布了博客后不满意进行删除, 之后又发布一篇标题相同的博客,这时候就会引发唯一索引带来的冲突.

解决方案为 :  对于使用了逻辑删除的表,唯一索引上必须要加上逻辑删除的字段.

但是这样设计同样有问题 , 如果用户删除两次相同标题的博客, 同样会引发唯一索引冲突.

最终解决方案为: 使用时间戳来表示已删除, 使用0来表示未删除.

## 海量数据下大页码查询

对于一个海量数据表, 执行下列sql

```sql
select * from table order by create_time limit 50000,10;
```

为了解决查询慢, 通常会给create_time增加索引,  但是实际并未生效, 因为没有使用where条件,   并且在create_time创建的索引下因为查询结果为*还要回表, 可能查询优化器认为走create_time执行效率比全表扫描还要慢, 所以走了全表扫描(explain的type 为ALL, Extra为Using filesort) ,在大数据量下这个执行效率是非常差的.

所以优化的主要方向为如何利用create_time索引?

A: 可利用索引覆盖特性查找第50000页的起始时间, 基于索引快速定位, 向后取10条数据:

```sql
select * from table where 
create_time >=
(
	select create_time from table 
	oreder by create_time limit 50000,1
)
order by create_time limit 10;
```

在这里, 子查询通过索引覆盖查出50000页的第一条数据, 效率是很高的(explian的type为index, extra为Using index), 接下来外层查询在order by和where下走了范围查询, 执行效率也得到了提高(explain的type为range,extra为**Using where**(也要回表))



拓展: 如果在连续翻页的情况下, 可利用上一页最后一条记录的时间, 作为下一页的查询起始时间, 可以减少一次子查询 .

这也是为什么很多地方UI只有上一页和下一页 还有移动端上拉加载下一页的原因, 没有显示总页数和总数减少获取总数和计算页数的过程.

>  高并发下不可取, 因为时间会重复, 在对后续页查询需要额外增加偏移量处理

> 另外时间采用时间戳效果会更好,不容易重复

实际上用户真的会点击10000页吗? 对于这些分页问题, 实际业务场景会有更好的策略. 而淘宝页面采用的是只支持最多查询到100页的方案.





## Redis分片下的缓存访问倾斜

redis分片下, 商品缓存均匀分布在redis分片 但是用户访问可能倾斜在某些热点商品,导致某些redis分片访问较多,其他的较少, 如何解决?

1.表示热点数据, 对热门数据单独分片.(缺点: 成本大)

将热点数据缓存集群与全量数据缓存集群分开 , 当请求访问热点数据时携带热点标识 , 应用根据热点标识决定访问哪个缓存.

2.缓存前置(客户端缓存) + 闪电缓存(进程内缓存)

在客户端将数据缓存到本地并设置较短有效期(主动实现)

在应用系统中增加有效期**极短** (减小不一致时间窗口 ,将损失拉到最小, 比如5s) 的进程内缓存,如果不存在再访问全量缓存分片.

## mysq时间格式选择

1.不要选择字符串存储日期;

```
字符串占用的空间更大！
字符串存储的日期效率比较低（逐个字符进行比对），无法用日期相关的 API 进行计算和比较。
```

2.Datetime 和 Timestamp

通常选择Timestamp,因为**Timestamp 和时区有关**。Timestamp 类型字段的值会随着服务器时区的变化而变化，自动换算成相应的时间，说简单点就是在不同时区，查询到同一个条记录此字段的值会不一样。且Timestamp 只需要使用 **4 个字节**的存储空间

而**DateTime 类型是没有时区信息的（时区无关）** ，DateTime 类型保存的时间都是当前会话所设置的时区对应的时间。这样就会有什么问题呢？当你的时区更换之后，比如你的服务器更换地址或者更换客户端连接时区设置的话，就会导致你从数据库中读出的时间错误。且DateTime 需要耗费 **8 个字节**的存储空间.

> Timestamp范围只有 1970-01-01 00:00:01 ~ 2037-12-31 23:59:59

> SELECT @@session.time_zone;  # 查看当前会话时区 
>
> SET time_zone = "+00:00"; #设置当前会话时区
>
> SELECT @@global.time_zone; # 全局时区
>
> SET GLOBAL time_zone = '+8:00';

3.当需要存储比秒更小时间粒度的时间用BIGINT类型存储微妙级别的时间戳,或者使用double存储秒之后的小数部分.

时间戳不存在时区问题

# 业务需求

## 前后端分离开发中动态菜单实现

1.**后端动态返回(常用)**

用户登录后后端数据基于RBAC获取菜单列表json返回给前端, 前端根据json动态渲染 .

```
[
    {
        "id":2,
        "path":"/home",
        "component":"Home",
        "name":"员工资料",
        "iconCls":"fa fa-user-circle-o",
        "children":[
            {
                "id":null,
                "path":"/emp/basic",
                "component":"EmpBasic",
                "name":"基本资料",
                "iconCls":null,
                "children":[

                ],
                "meta":{
                    "keepAlive":false,
                    "requireAuth":true
                }
            }
        ],
        "meta":{
            "keepAlive":false,
            "requireAuth":true
        }
    }
]
```

在前端的二次处理主要是把 component 属性的字符串值转为对象。 

处理后端返回的路由json

```
export const formatRoutes = (routes) => {
    let fmRoutes = [];
    routes.forEach(router => {
        let {
            path,
            component,
            name,
            meta,
            iconCls,
            children
        } = router;
        if (children && children instanceof Array) {
            children = formatRoutes(children);
        }
        let fmRouter = {
            path: path,
            name: name,
            iconCls: iconCls,
            meta: meta,
            children: children,
            component(resolve) {
                if (component.startsWith("Home")) {
                    require(['../views/' + component + '.vue'], resolve);
                } else if (component.startsWith("Emp")) {
                    require(['../views/emp/' + component + '.vue'], resolve);
                } else if (component.startsWith("Per")) {
                    require(['../views/per/' + component + '.vue'], resolve);
                } else if (component.startsWith("Sal")) {
                    require(['../views/sal/' + component + '.vue'], resolve);
                } else if (component.startsWith("Sta")) {
                    require(['../views/sta/' + component + '.vue'], resolve);
                } else if (component.startsWith("Sys")) {
                    require(['../views/sys/' + component + '.vue'], resolve);
                }
            }
        }
        fmRoutes.push(fmRouter);
    })
    return fmRoutes;
}
```

添加路由到router

```
 let fmtRoutes = formatRoutes(data);
router.addRoutes(fmtRoutes);
```



> 若依, ELADMIN, 微人事等项目均采用此方式

2.**前端动态渲染**

直接在前端把所有页面都在路由表里边定义好，然后在 meta 属性中定义每一个页面需要哪些角色才能访问，例如下面这样：

```
[
    {
        "id":2,
        "path":"/home",
        "component":Home,
        "name":"员工资料",
        "iconCls":"fa fa-user-circle-o",
        "children":[
            {
                "id":null,
                "path":"/emp/basic",
                "component":EmpBasic,
                "name":"基本资料",
                "iconCls":null,
                "children":[

                ],
                "meta":{
                    "keepAlive":false,
                    "requireAuth":true,
                    "roles":['admin','user']
                }
            }
        ],
        "meta":{
            "keepAlive":false,
            "requireAuth":true
        }
    }
]
```

这样定义表示当前登录用户需要具备 admin 或者 user 角色，才可以访问 EmpBasic 组件，当然这里不是说我这样定义了就行，这个定义只是一个标记，在项目首页中，我会遍历这个数组做菜单动态渲染，然后根据当前登录用户的角色，再结合当前组件需要的角色，来决定是否把当前组件所对应的菜单项渲染出来。

> 弊端就是菜单和角色的关系在前端代码中写死了

## GIS文件检索系统

```
需求:
1.公司希望开发一套文件检索系统,承载海量地理信息文件,提供多维度的数据检索,要求这个系统能提供高可用与高并发性能.
2.每一组GIS地图瓦片(分片)都是JSON文件(元数据)+若干图片+二进制地理数据文件,尺寸均小于100k.
3.这些地理数据比较稳定,不定期会产生小规模更新.
4.目前积累的数据量约为1T, 目测文件数量最少上千万.
5.应用UV(UserAccess)很低,日均100-200访问,但QPS并发了很高,因为一张地图要切分十几(百)个瓦片,在访问时并发获取.
```

核心复杂度:

JSON数据如何进行多维度检索; 上千万JSON数据查询是否存在性能瓶颈; 高并发下上千万文件磁盘IO是否扛得住; 如何保证数据库与磁盘是可靠的; 是否存在带宽等附加问题.

1.JSON的存储与解析:

MySQL8支持存储解析JSON, 但不专业; 可使用支持上亿海量数据的MongoDB(可自动解析JSON), 并基于集群分片保障数据可靠性.

2.文件存储:

数据库存储的是文件的地址, 提取还是要从硬盘, 上千万文件的随机读光寻址都要等很久, 所以选择SSD来支持随机读, 缺点成本高.

3.存储高可用:

1T存储空间不大(在UV极小的情况下, QPS在IO上也是小问题). 可采用下列方式:

Raid 10 : 存在50%冗余;

Raid 5 : 预算有限时可采用, 部分冗余

如果数据服务器本机部署, 虽然不用考虑网络带宽问题, 但所有磁盘都挂载在一台服务器上在物理上会形成单点.

可将应用服务与文件存储分开, 文件存储更是可采用分布式文件系统(建议FastDFS , Hadoop HDFS存储小文件效率比FastDFS差很多).

4.带宽

如果一块地图平均需要25个瓦片数据, 每个瓦片100KB, 如果处理时间相同情况下, 数据同时返回, 每个用户需要25 * 100KB * 8 = 20000kbps 即 20m的带宽, 这在局域网兴许可以, 但在广域网传输显然带宽占用太多,  还需要额外考虑资源压缩传输.

# 规约

## 为什么不使用外键

> 【阿里JAVA规范】不得使用外键与级联，一切外键概念必须在应用层解决。

1.每次做DELETE 或者UPDATE都必须考虑外键约束，会导致开发的时候很痛苦,测试数据极为不方便。

2.性能问题, 在外键关联表插入数据是会带来额外的数据一致性校验查询 .

3.并发问题,外界约束会启用行级锁 ,主表写入时外键关联表会进入阻塞.

4.级联删除问题, 多层级联删除会让数据变得不可控, 触发器也严格被禁用.

5.数据耦合, 数据库层面数据关系产生耦合, 数据迁移维护困难.

## 禁止三表join

>【强制】超过三个表禁止join . 需要join的字段 , 数据类型必须绝对一致 ; 多表关联查询时, 保证被关联的字段需要有索引。
>
>说明: 即使双表join也需要注意表索引,sql性能.

产品强制要求: 阿里OceanBase(MySql改造) 只允许两表关联 , MyCat只支持两表关联.

原因:

1.Mysql自身设计缺陷, 超过三表关联时优化器做的不好 ; 

> 银行业务无此需求 , oracle无mysql的此缺陷

2.NLJ( 嵌套循环关联 )多级嵌套性能差, 贪心与动态规划算法计算量激增;

> NLJ采用小表关联大表(小表作为驱动表,大表作为匹配表)

3.依赖数据源特性获取数据, 数据迁移改造难;

比如有商品库和订单库,一条商品记录对应对条订单记录.

当订单量巨大时, 可能需要单独迁移出一个模块分开维护, 这时候商品服务需要restful|RPC调用订单服务,  此时单条join无法完成; 

或者当订单量巨大时 需要分表存储 , 此时单条join也无法完成 ,需要使用多条in语句进行查询(在商品表中查询出所有订单表的对应信息, 然后在每个订单表分片中使用in 查询) , 而使用in只适用于数据量小(最大1000),且只支持inner join.

解决方案

反范式表:将所有关联的表拼接成一个完整的大表(数据冗余) .

在数据变更时, 将数据插入反范式表中 , 但随着数据增多, 该表同样会变得臃肿.

> 为什么不使用视图 ,视图只是逻辑表 ,查询时还是需要结合多张表查询

**最终解决方案 数据集市**

通过ETL(抽取&转换&加载)每天(T+1日终处理,第二天凌晨)对数据源进行抽取 , 在数据仓库中进行加工处理生成额外的表.

> 在ETL数据集市中,通常还会使用生成倒排表的形式关联, 比如 在对订单明细表分片时, 不根据订单明细id进行分片, 而是根据订单id进行分片, 当需要获取指定订单id的订单明细时,  不需要遍历所有分片, 只需要对订单id取模获取其对应订单明细的分片.
>
> 通过每次日终额外生成倒排表提高数据提取效率 , 典型的空间换时间.

## 禁止使用存储过程

> 【强制】禁止使用存储过程,  存储过程难以调试和扩展, 更没有移植性。

1.为什么银行都在使用存储过程?

银行业务以数据为核心 , Oracle、DB2一统江湖，存储过程与语言无关 , 预算充足，好多个W采购小型机满足性能要求 ,存储过程几乎是每一个信息科技处开发员工的入职要求. 而银行被Oracle、DB2绑架 , 数据好迁移，存储过程要全部重写  ,谁来/谁敢承担核心业务的风险？

![image-20211206203335275](picture/image-20211206203335275.png)

2.**存储过程在互联网分布式场景的问题**

① 分片场景下存储过程只能作用在局部数据(本分片);

② 数据库压力激增 

③ 无法保证分布式全局事务

![image-20211206204754565](picture/image-20211206204754565.png)

## 禁止左模糊, 搜索引擎替代

> 【强制】页面搜索严禁左模糊或者全模糊, 如果需要请走搜索引擎来解决
>
> 说明: 索引文件具有B-Tree的最左前缀匹配特性, 如果左边的值未确定 ,那么无法使用索引

左匹配 `%xxx` 将不会使用索引 `using where`

**索引选择性(关键)**

mysql中的查询优化器 **会自动根据索引和数据分布情况决定是否走索引**

右匹配也不一定会使用索引: 如在大量数据的表下对手机字段使用 `1%`模糊匹配  或在程序员表的性别字段使用精确匹配,  这就是索引选择性差带来的全表扫描.

> 在不严谨的解释下: 命中的索引值超过总量的25% ,就可能产生索引选择性陷阱, 导致全表扫描.
>
> 当然 , 一切还是以Explain执行计划为准.

可通过组合索引提高选择性 ,  如果实在不能组合 , 对于**全文检索**就只能使用ES或Solr了(ES基于分片多线程检索, 解决查询慢的问题)

但加入ES就引发了后续架构复杂度 , 数据一致性 , 分布式.

另外, 单从mysql索引选择性差的问题上的解决方案也有:

1.强制使用索引( 以需要根据优化器以实际运行效果为准),如对选择题答案过滤时

```
explain select * from question force index(answer) where answer = 'A'
```

2.增加缓存, 提高全表扫描速度(钞能力), 更可加入redis.

```
innodb_buffer_pool_size=16G
innodb_buffer_pool_instances=2
```

# 高并发下的动静分离

动静分离是架构三大分离设计(读写分离,动静分离,前后台分离)之一

静态数据是”无个性化”数据

● 静态文件: HTML/CSS/JS/图片

● 低频变动数据: 字典数据 / 地区数据 / 组织架构 / 历史数据

动态数据就是个性化/高频写数据

● 个性化推荐 

● 高频写: 股市行情 / 5G信号数据 / 天气变化

> **有效区分页面中的动静数据是优化的关键前提**

 ![image-20211209211336579](picture/image-20211209211336579.png)

> 静态缓存通常可通过CDN,Nginx,Varnish等实现, [可见](note/guide/distributed.md)

1.页面静态化技术

 将动态页面”另存为”静态页面保存到本地磁盘

 利用Nginx直接路由到磁盘文件,不再进入后端

 文件碎片化严重,文件同步管理麻烦

2.页面伪静态化技术(推荐)

 利用Redis缓存,缓存生成的页面

 没有碎片化问题,可自动过期,数据管理轻松

 需要大量内存存储信息

![image-20211209212113051](picture/image-20211209212113051.png)

但是, 这种静态化不是万能的.

●静态化只适合数据集有限(百万量级)的场景

热点商品SKU

●页面集过大不适合静态化

商品全量SKU,文件碎片太多

批量同步磁盘IO瓶颈

伪静态化内存开销太大

●动态内容是静态化遇到的新挑战



**动静整合方案**

1.**服务端SSI法**

利用Nginx SSI特性实现服务端动静整合

SSI是Server Side Inclde的缩写，是一种基于服务端的网页制作技术，就是服务端包含的意思，该项目中用到了nginx中SSI模块的include命令，这个命令会包含一个页面，然后在nginx服务器中展开。

Nginx中SSI的开启:

```
在安装路径下的conf文件夹中的nginx.conf文件中添加如下三行代码,开启SSI
ssi on;
ssi_silent_errors on;
ssi_types text/html;
```

使用Nginx结合lua脚本通过判断文件路径前缀 , 分别进行动静访问.

2.Ajax异步调用法

生成静态页面,动态数据部分发起Ajax异步查询





# JDK序列化问题

目前springboot , springcloud下 ,都是默认使用基于HTTP的json作为序列化的首选方案.

而JDK内置的序列化却遭到抵制.

因为这是java 利用自带api生成序列化二进制流, 所以接收端只能强制使用java 开发才能识别, 失去了异构系统的跨平台性.

> JDK序列化可通过ByteArrayXXXputStream和ObjectXXXputStream实现

连java官方也说明:" 对不信任数据的反序列化 , 从本质上来说是危险的 , 应该予以避免" . 可见java序列化是不安全的.

比如在网络传输过程(网络安全由应用程序自己规定)中, 传输的序列化后的二进制流金额数据被注入攻击修改.

而基于HTTP的RESTFul风格传输的话, 数据采用json传递, 可使用https的SSL加密传输 ,可保证过程的安全.



另外, , 还有NIO的ByteBuffer序列化方式 , 在序列化执行效率上面 , 与java序列化差距不大.

但json序列化执行效率比java序列化更高(长度和速度上) , 因为java序列化需要处理对象的引用等.

> 附上其他序列化方案 Dubbbo内置的hessian 和 Google的Protocol Buffers

> 一些情况 如Spring session用Redis做session共享时 和 spring cache 用redis做缓存时  ,默认都是使用的jdk序列化
