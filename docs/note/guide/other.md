# 数据业务与场景

##  数据业务

### 秒杀下的防止超卖

特征:秒杀商品库存总量固定 , 先到先得，瞬间并发极大，但写库量有限.

利用`redis实现计数器 与 decr原子操作` +` nginx 与 lua脚本` 完成.

![image-20211210205559293](picture/image-20211210205559293.png)

![image-20211210205955691](picture/image-20211210205955691.png)

主要关键点:

1. 利用预减库存方式杜绝超卖

2. 利用Nginx+Lua+Redis在网关层面将无效请求阻挡

3. 利用MQ消息队列的QoS限流特性保证MySQL不会被瞬间击垮

4. APP需要额外设计轮询机制查询订单状态(类似使用银行卡转账后的转圈圈)

>TIPS: 订单创建后，用户取消订单或未支付怎么办？
>
>为订单设置过期时间，订单支付有效期过期或用户取消订单 . LUA执行 incr “stk-1001” 自增即可







### 乐观锁解决并发数据冲突

> 悲观锁性能差,用户体验差 ,且现在主流场景如电商都是读多写少

通过给表添加version字段实现, 每次修改前先查询出版本,修改数据时先判断版本并版本加一.

如果遇到冲突怎么办?

1. 前端应用提示"数据正在处理 , 请稍后再试";
2. 附加spring-retry在service上进行方法重试

```
@Transactional
//重试异常  最大重试次数  重试间隔默认1S
@Retryable(value = {VersionException.class}, maxAttempts = 3)
public void updateBal(){
    Account acc =  执行：”select id,bal,_version from acc where id = 1001”;
    acc.setBal(acc.getBal() + 400)；
    int count = 执行：“update acc set bal = ${acc.bal} , _version=_version + 1 
		where id = 1001 and _version=${acc.version}”；
    if(count == 0) { throw new VersionException(“产生并发异常”) }；
}
```

> 问题: 为什么不直接一条语句update即可?   尽量不要将计算放在sql语句中

### 高并发下的动静分离

动静分离是架构三大分离设计(读写分离,动静分离,前后台分离)之一

静态数据是”无个性化”数据

● 静态文件: HTML/CSS/JS/图片

● 低频变动数据: 字典数据 / 地区数据 / 组织架构 / 历史数据

动态数据就是个性化/高频写数据

● 个性化推荐 

● 高频写: 股市行情 / 5G信号数据 / 天气变化

> 有效区分页面中的动静数据是优化的关键前提, 比如淘宝商品的图片, 标题, 参数等为静态数据, 商品价格, 库存为动态数据.

> 静态缓存通常可通过CDN,Nginx,Varnish等实现, [可见](note/guide/distributed.md)

1.页面静态化技术

 将动态页面”另存为”静态页面保存到本地磁盘

 利用Nginx直接路由到磁盘文件,不再进入后端

 文件碎片化严重,文件同步管理麻烦

2.页面伪静态化技术(推荐)

 利用Redis缓存,缓存生成的页面

 没有碎片化问题,可自动过期,数据管理轻松

 需要大量内存存储信息

![image-20211209212113051](picture/image-20211209212113051.png)

但是, 这种静态化不是万能的.

●静态化只适合数据集有限(百万量级)的场景

热点商品SKU

●页面集过大不适合静态化

商品全量SKU,文件碎片太多

批量同步磁盘IO瓶颈

伪静态化内存开销太大

●动态内容是静态化遇到的新挑战



**动静整合方案**

1.**服务端SSI法**

利用Nginx SSI特性实现服务端动静整合

SSI是Server Side Inclde的缩写，是一种基于服务端的网页制作技术，就是服务端包含的意思，该项目中用到了nginx中SSI模块的include命令，这个命令会包含一个页面，然后在nginx服务器中展开。

Nginx中SSI的开启:

```
在安装路径下的conf文件夹中的nginx.conf文件中添加如下三行代码,开启SSI
ssi on;
ssi_silent_errors on;
ssi_types text/html;
```

使用Nginx结合lua脚本通过判断文件路径前缀 , 分别进行动静访问.

2.Ajax异步调用法

生成静态页面,动态数据部分发起Ajax异步查询

### 使用代理主键

为什么表的主键要使用代理主键（如无意义的自增编号），不建议使用业务主键(如身份证)?

常规上来说

1. 业务主键更浪费空间

2. 业务主键无顺序，可能会造成写入数据时需要更长组织索引

3. 业务主键如果是字符串，在分库分表时，无法直接取模运算，需要先转换为数字，处理更麻烦了。

业务架构上来说:

1. 有时遇到需要修改业务主键的情况, 而如果分布式下多个模块依赖到此主键, 则带来了很麻烦的连锁反应
2. 对于目前常使用的逻辑删除情况下,  如果要再次插入已逻辑删除的数据 , 而因为使用了逻辑主键, 出现了主键冲突 .

> 不直接修改已逻辑删除的数据 , 是因为大多数业务不允许将删除前后数据关联起来 , 并且要存留删除后的信息 , 如员工信息

### 逻辑删除与唯一索引

**逻辑删除:**实际业务中, 对删除的数据给它设置一个标记"已删除", 在后续正常查询中,会过滤这条数据. 

有利于排查问题与补救.

通常设置标记的字段数据类型有布尔类型(mysql中可以使用tinyint) ; 字符串类型(y/n).

而逻辑删除与唯一索引共同使用时, 比如博客系统中,标题+作者是唯一的, 用户发布了博客后不满意进行删除, 之后又发布一篇标题相同的博客,这时候就会引发唯一索引带来的冲突.

解决方案为 :  对于使用了逻辑删除的表,唯一索引上必须要加上逻辑删除的字段.

但是这样设计同样有问题 , 如果用户删除两次相同标题的博客, 同样会引发唯一索引冲突.

最终解决方案为: 使用时间戳来表示已删除, 使用0来表示未删除.

### 海量数据下大页码查询

对于一个海量数据表, 执行下列sql

```sql
select * from table order by create_time limit 50000,10;
```

为了解决查询慢, 通常会给create_time增加索引,  但是实际并未生效, 因为没有使用where条件,   并且在create_time创建的索引下因为查询结果为*还要回表, 可能查询优化器认为走create_time执行效率比全表扫描还要慢, 所以走了全表扫描(explain的type 为ALL, Extra为Using filesort) ,在大数据量下这个执行效率是非常差的.

所以优化的主要方向为如何利用create_time索引?

A: 可利用索引覆盖特性查找第50000页的起始时间, 基于索引快速定位, 向后取10条数据:

```sql
select * from table where 
create_time >=
(
	select create_time from table 
	oreder by create_time limit 50000,1
)
order by create_time limit 10;
```

在这里, 子查询通过索引覆盖查出50000页的第一条数据, 效率是很高的(explian的type为index, extra为Using index), 接下来外层查询在order by和where下走了范围查询, 执行效率也得到了提高(explain的type为range,extra为**Using where**(也要回表))

拓展: 如果在连续翻页的情况下, 可利用上一页最后一条记录的时间, 作为下一页的查询起始时间, 可以减少一次子查询 .

这也是为什么很多地方UI只有上一页和下一页 还有移动端上拉加载下一页的原因, 没有显示总页数和总数减少获取总数和计算页数的过程.

>  高并发下不可取, 因为时间会重复, 在对后续页查询需要额外增加偏移量处理

> 另外时间采用时间戳效果会更好,不容易重复

实际上用户真的会点击10000页吗? 对于这些分页问题, 实际业务场景会有更好的策略. 而淘宝页面采用的是只支持最多查询到100页的方案.



### Redis分片下的缓存访问倾斜

redis分片下, 商品缓存均匀分布在redis分片 但是用户访问可能倾斜在某些热点商品,导致某些redis分片访问较多,其他的较少, 如何解决?

1.标识热点数据, 对热门数据单独分片.(缺点: 成本大)

将热点数据缓存集群与全量数据缓存集群分开 , 当请求访问热点数据时携带热点标识 , 应用根据热点标识决定访问哪个缓存.

2.缓存前置(客户端缓存) + 闪电缓存(进程内缓存)

在客户端将数据缓存到本地并设置较短有效期(主动实现)

在应用系统中增加有效期**极短** (减小不一致时间窗口 ,将损失拉到最小, 比如5s) 的进程内缓存,如果不存在再访问全量缓存分片.

### Redis大Key带来的问题

某小说网站在小说详情页面存在`谁读过这本书`的显示功能, 为了提高速度,进行了缓存优化, 以bookId为key, 保存读过这本小说的所有用户uid的set为value, 并将uid对应用户信息对象也缓存了起来, 其中用户信息中记录了该用户已读的小说列表. 

初期优化效果显著, 但随着用户量不断的增加, 比如到了300W, 该功能的API QPS 从1100降低至200,  多方排查后发现是大Key(超过512Kb)导致执行效率大幅度降低.

大Key会导致Redis内存浪费, 频繁LRU清除, 间接导致缓存穿透; 如果是集群模式还会存在节点倾斜问题, 负载不均衡; 单数据提取慢, 客户端指令队列积压严重.

解决方案:

1.裁剪. 在业务方面, 因为该功能实际使用的set元素很少, 没必要全量缓存(最多100个即可), 并且用户缓存也不要缓存过多数据, 只缓存用户对象中基本的信息即可(如昵称, 头像等), 没必要缓存包括已读列表的整个后端对象;

2.开启Redis6客户端缓存或其他本地缓存(如ehcache), 存在一致性问题, 但该业务功能允许存在延迟

### 索引选择性

使用索引并不一定能降低SQL执行时间.

MySQL用到了索引和执行时间没有必然关系, 确定查询执行效率的是`扫描行数`与`回表次数`.

```sql
create table t (
	id int(11) not null,
	a varchar(64) default null.
	b int(11) default null,
	primary key (id)
	key a(a)
)engine-in
```

对于上面的表

```sql
explain select * from t ;
#全表扫描: type=ALL 不会使用索引

explain select * from t where id = 2;
#主键索引快速过滤 type=const key=PRIMARY rows=1

explain select a from t
#索引覆盖,无需回表 type=index key=a
```

特例

```sql
explain select * from t where id > 0;
#虽然key=PRIMARY 但是rows为实际所有行数
#基于id扫描全表, 且产生大量回表, 速度慢
```

联合索引, 假设t表有1亿条数据, 其中600万姓张:

```sql
create index t on (a,b);

explain * from t where a='张三' and b=23
#索引选择性较好

explain * from t where a like '张%' and b=23
#索引选择性较差
#如果姓张的人超过一个比例mysql可能不会走索引而全表扫描
#5.6以前基于a列对'张%'进行筛选然后回表对b列23进行筛选,速度较慢
#5.6以后基于联合索引对a筛选'张%'再对b过滤23,再将复合条件的id进行回表提取(索引下推)
```

> 索引选择性:mysql中的查询优化器 会自动根据索引和数据分布情况决定是否走索引, 可通过show index的cardinality与实际行数比较来评估

如何提高索引选择性?

1.增加多种不同规格索引提高索引选择性;

2.空间换时间, 定时任务增加时报, 日报, 月报等中间结果;

3.边缘运算数据压缩,比如物联网中对未出异常前的n条数据进行压缩, 使用时查询后解压, 异常数据不压缩(常用);

4.硬件调优, 增大innodb_buffer_pool, 多利用内存, 减少硬盘回表;

5.分库分表等

### 多个范围查询如何利用索引

对于索引(sex,country,last_login,age), 某相亲app用户如果想查询性别男, 中国, 年龄大于20小于25的最近7天登录过的用户.

```sql
where sex = '男'
and country = '中国'
and last_login > DATE_SUB(NOW(),INTERVAL 7 DAY)
and age between 20 and 25
```

因为存在两个范围查询, 所以想同时使用两个索引是不可能的, 只能寻找其他途径.

可通过新增一个列active, 用户每次登录将active设为1, 然后通过定时任务修改7天未登录过的用户active为0.

然后通过索引(sex,country,active,age)就可以进行查询啦

```sql
where sex = '男'
and country = '中国'
and active = 1
and age between 20 and 25
```

### MySQL时间格式选择

1.不要选择字符串存储日期;

```
字符串占用的空间更大！
字符串存储的日期效率比较低（逐个字符进行比对），无法用日期相关的 API 进行计算和比较。
```

2.Datetime 和 Timestamp

通常选择Timestamp,因为**Timestamp 和时区有关**。Timestamp 类型字段的值会随着服务器时区的变化而变化，自动换算成相应的时间，说简单点就是在不同时区，查询到同一个条记录此字段的值会不一样。且Timestamp 只需要使用 **4 个字节**的存储空间

而**DateTime 类型是没有时区信息的（时区无关）** ，DateTime 类型保存的时间都是当前会话所设置的时区对应的时间。这样就会有什么问题呢？当你的时区更换之后，比如你的服务器更换地址或者更换客户端连接时区设置的话，就会导致你从数据库中读出的时间错误。且DateTime 需要耗费 **8 个字节**的存储空间.

> Timestamp范围只有 1970-01-01 00:00:01 ~ 2037-12-31 23:59:59

> SELECT @@session.time_zone;  # 查看当前会话时区 
>
> SET time_zone = "+00:00"; #设置当前会话时区
>
> SELECT @@global.time_zone; # 全局时区
>
> SET GLOBAL time_zone = '+8:00';

3.当需要存储比秒更小时间粒度的时间用BIGINT类型存储微妙级别的时间戳,或者使用double存储秒之后的小数部分.

时间戳不存在时区问题

## 场景

### 12306

在数据存储与查询上面

> 读扩散: 购票扣减只需要关注该列车对于该票上起点和目标站点的票的扣减, 关注车次.
>
> 读扩散优点: 扣减简单.   读扩散缺点: 计算余票复杂.
>
> 写扩散: 购票扣减直接对各个相关站点进行扣减, 关注站点.
>
> 写扩散优点: 扣减复杂.   写扩散缺点: 查询余票简单.

理论上对于读多写少的12306场景, 写扩散更加适合, 但实际上12306采用的是读扩散, 每次查询进行动态计算, 在引进了GemFire(内存数据库)之后, 巨大并发计算查询的瓶颈得到了解决, 其最大的特点为将**计算和存储都放在了内存**, 这也是其与redis(速度再快, 也只能取出来CPU计算)的区别.

GemFire基于Java开发, 将所有数据存放到内存, 聚集多台机器的内存汇成一个大节点整体管理, 并尽量保证业务运算与数据存储在同样一个节点, 避免多节点通信. 其内部通过Locator组件将请求路由到具有最新数据的Region. 并实现强一致性, 虽然用户感觉还是最终一致性, 因为有余票并不一定能买到. 

1.分时段售票、 候补抽票降低流量峰值

2.用复杂验证码解决灰色流量(抢票软件);

3.云计算弹性扩容解决读多写少的问题(实际上12306的上限已经确定, 并不需要频繁地去扩容, 不管是MQ,Redis, ES, 扩容都是相当复杂的);

4.在中间站点火爆的情况下, 需要预留两端距离更远的票;

### 西安健康码

健康码, 核酸检测报告无法打开.用户不断刷新, 增加了服务器的压力.

优化点:

1.客户端采取合并请求处理, 比如n秒内只能发出一个请求, 按钮点击以后倒计时;

2.避免实时运算, 对于与核心内容无关的静态数据, 可使用CDN+本地缓存兜底, 或友好提示;

3.因为全部用户都无法打开, 所以没有采取限流, 需要重点考虑阈值(大于需求小于极限),  在压力测试的时候需要持续测试, 因为如线程池、MQ等都是持续占用堆积的过程;

4.多级缓存(本地+集中式), 配合cache aside + 延迟双删, 为了防止缓存穿透, 可缓存null值, 不一致时间窗格为数据库数据库内容改变到缓存过期, 这个延迟是可以允许的,  因为健康码和核算检查其实晚一点获取到. 为了防止缓存击穿, 可阻塞其他线程只让一个线程去读取数据库. 为了防止缓存雪崩, 可随机设置超时时间;

5.两个功能同时无法提供服务, 可能服务没有做拆分, 通过微服务可以做故障隔离;

6.sql优化, 避免大事务;

7.实现APP无感知的后端扩容, 可使用容器技术动态扩容(有钱且有能力) 或 keepalived主从+智能DNS(可根据线路或区域等映射IP)实现互为主从保证高可用以及增加带宽; 

8.为了保持主从数据库一致性可采用半同步然后异步防止数据丢失(如果缓存设计恰当, 数据库的一致性设计不需要太复杂, 或直接使用redis持久化而丢弃数据库?);

9.增加监控系统, 系统性能预警(Prometheus, arths等),  Prometheus通过在各分布式机器上部署指标收集器exporter抽取指标信息存到本地tidb中, 再通过Grafana进行可视化分析, Prometheus还可以根据预定的规则推送警告信息通过AlertManager以各种方式发送给维护人员;

10.数据查询应用生成二维码JSON数据, 二维码图片在客户端APP动态解析生成;

11.redis集群模式通过crc16对身份证运算分配到不同的槽位,  对于redis集群模式, 不要利用其自带的转发模式, 因为转发节点压力大, 转发节点崩了就不可用了,  可以通过配置中心配置不同身份证运算后的范围应访问的redis机器ip.  另外, 如果需要动态扩容redis节点,再重新分配槽后, 可通过apollo配置中心的push模式动态修改配置文件;

> 如何估算平均QPS? 
>
> 估算QPS = ( 1000万(预估西安人口,除去老人与小孩) * 12(预估每天12次) ) / ( 16(减去睡觉时间) * 3600(秒) )
>
> = ( 10000000 * 12 ) / ( 16 * 3600 )  = 2085
>
> 如何根据估算QPS选择部署的web节点或redis节点等的数量?
>
> 可采用20倍均值原则计算需要的负载峰值能力,  并根据压测结果决定数量
>
> 节点数 = ( 平均QPS * 20 ) / (单机压测峰值 * 80% )
>
> 这里平均QPS * 20 表示峰值预估 , 这里为  2085 * 20  = 41700
>
> 为什么单机压测峰值要乘0.8?  因为不能刚好踩点进行节点数选择, 且提供20%的扩容时间余地 ( 可通过Prometheus监测总体压力超过80%时进行提醒去扩容)
>
> 假设单机服务器压测值为2000 QPS , 结果为
>
> 节点数 = 41700 / ( 2000 * 0.8 ) = 27台

> 如何估算缓存二维码数据的redis节点的带宽和容量?
>
> 假设一个二维码JSON为 1kb , redis节点4台
>
> 先看一下需要的总和带宽 = 41700 * 1kb * 8 = 326MB, 无论是QPS还是带宽, 这对于机器来说压力其实不大.
>
> redis节点内存 = 10000000人 * 1kb / 4台 = 2440mb = 2.3gb 

### 红包雨

场景: 1亿金额, 10亿个红包, TPS预计百万级.

1.TPS过大, 流量入口如果为同一个, 如nginx( Linux下一台nginx通常最大5w/s), 肯定顶不住, 可考虑LVS(十万级)、 F5(百万级)商用设备;

2.活动开始前将红包分配给不同地区的机房, 通过HTTP/DNS, 将用户的请求落到本地机房处理, 最后MQ同步到账户服务, 更新钱包(有延迟);

3.本地TPS可能还是有十万级, 本地也需要高性能, 服务器集群可根据TPS冗余部署, 以redis为例, 如果本地TPS为50W, 每台redis 8W, 需要7台, 可冗余部署到10台, 在应用层做轮询, 挂了需要剔除;

4.每个redis实例存储一个 `红包id+红包金额`的List, 抢红包时可使用Lua脚本从List中pop一个元素, 同时记录到另一个 `用户id + 红包id + 红包金额`的list, 最后通过定时任务不断从redis集群中取出用户id和金额数据, 批量插入到Mysql集群, 同时通过MQ或活动结束后同步到账户服务;

5.Mysql可通过用户id进行分库, 以支持通过用户id查询其红包列表;

6.红包拆分算法, 需要保证公平, 红包不能为0;

> 二倍均值法: 每次红包金额 = random( 0, 剩余红包金额 / 剩余人数 * 2)
>
> 除了最后一个红包, 其他任何一个红包不会超过人均金额的两倍. 实现简单, 空间复杂度低.
>
> 线段分割法: 红包总额为线段，而每个红包金额为这条主线段所拆分出的若干子线段。(N-1个切割点)
>
> 实现较复杂, 红包差额可能出现较大的情况, 可通过多线程再分段优化性能处理.

7.风控防刷, 与高并发相矛盾, 因为每次请求都需要走一遍风控规则判断, 需要进行权衡. 可在同步到账户前异步进行风控规则;

8.另外需要考虑高可用, 无状态服务弹性伸缩, 压测和限流等.

### 银行交易系统隔离

场景:交易系统安全级别高, 需要独立部署. 而交易数据需要被外围系统获取, 交易系统不能直接开放接口.

![image-20220213180003277](picture/image-20220213180003277.png)

1.因为银行外围系统对交易实时性要求不高, 可采用T+1数据获取. 银行通常使用oracle数据库, 在每天凌晨发起任务调度脚本, 通过oracle自带的SqlLoader数据导出工具导出为csv文件并推送到独立的FTP服务器上.

> CSV为逗号分割数据格式的文件

2.FTP服务器提供交易核心系统写权限, 外围读权限.

3.通过ETL(导出&转换加工&导入)工具读取CSV文件导入到ODS中. 并提供ETL日志, 用于错误发现,

> ODS又称数据中台或数据仓库, 用于存储分析处理后的数据.

所有外围系统只能面向ODS获取数据, 而无法接触核心系统, 保障了数据隔离

### PC扫码登录

**微信手机端登录流程**:

1.微信输入手机号+验证码发送给服务器

2.服务器校验验证码并绑定设备, 最终生成token(手机号, 设备唯一标识, 操作系统, 其他唯一标识)返回

3.手机端通过请求头携带token调用服务器API

4.服务器验证token格式, 校验token与设备(唯一标识)是否对应(防止盗用), 验证是否具有该API访问权限. 如都满足, 则放行



**微信PC端扫码登录流程**

阶段一: 二维码生成

1.用户打开PC端微信登录界面, 微信发送PC唯一标识(如设备标识、 MAC等)

2.服务器生成PC端**二维码id并与PC设备绑定**, 然后返回给PC端, 此时二维码状态为`待扫描`

3.PC端根据二维码id生成二维码显示, 并**定时轮询二维码状态**

> PC端轮询二维码状态为`待扫描`时显示二维码

阶段二: 手机扫码

1.用户通过手机微信扫描PC端二维码, 获取二维码id

2.手机微信app将**手机token**(微信手机端登录生成的)与**二维码id**发送给服务器

3.服务器将手机token与二维码id进行绑定并生成**临时token**, 然后返回给手机端, 更新二维码状态为`已扫描`

> 临时token用于保证扫码的手机和确认登录的为同一设备, 防止伪造.

> PC端轮询二维码状态为`已扫描`时显示用户简单信息(头像与昵称), 并提示`需在手机上确认登录`

阶段三: 确认登录

1.用户通过手机微信点击`确认登录`, 请求头附加临时token给服务器

2.服务器验证临时token与设备是否对对应, 然后**销毁临时token生成正式token**(与PC设备绑定), 然后响应手机端登录成功, 更新二维码状态为`确认登录`

3.手机端收到登录成功响应显示操作成功

4.此时PC端定时轮询二维码状态发现二维码状态为`确认登录`, 从服务器提取PC正式token.

5.PC端微信通过请求头访问服务器API

### 秒杀

秒杀场景设计通常redis缓存goods_id为key, 库存为value, 再加goods_id为key, 售出用户列表为value, 每个用户在前面需要对库存进行判断是否大于0,  在并发场景下容易出现超卖现象.

可通过Lua脚本保证三次访问redis为原子操作

### GIS文件检索系统

```
需求:
1.公司希望开发一套文件检索系统,承载海量地理信息文件,提供多维度的数据检索,要求这个系统能提供高可用与高并发性能.
2.每一组GIS地图瓦片(分片)都是JSON文件(元数据)+若干图片+二进制地理数据文件,尺寸均小于100k.
3.这些地理数据比较稳定,不定期会产生小规模更新.
4.目前积累的数据量约为1T, 目测文件数量最少上千万.
5.应用UV(UserAccess)很低,日均100-200访问,但QPS并发了很高,因为一张地图要切分十几(百)个瓦片,在访问时并发获取.
```

核心复杂度:

JSON数据如何进行多维度检索; 上千万JSON数据查询是否存在性能瓶颈; 高并发下上千万文件磁盘IO是否扛得住; 如何保证数据库与磁盘是可靠的; 是否存在带宽等附加问题.

1.JSON的存储与解析:

MySQL8支持存储解析JSON, 但不专业; 可使用支持上亿海量数据的MongoDB(可自动解析JSON), 并基于集群分片保障数据可靠性.

2.文件存储:

数据库存储的是文件的地址, 提取还是要从硬盘, 上千万文件的随机读光寻址都要等很久, 所以选择SSD来支持随机读, 缺点成本高.

3.存储高可用:

1T存储空间不大(在UV极小的情况下, QPS在IO上也是小问题). 可采用下列方式:

Raid 10 : 存在50%冗余;

Raid 5 : 预算有限时可采用, 部分冗余

如果数据服务器本机部署, 虽然不用考虑网络带宽问题, 但所有磁盘都挂载在一台服务器上在物理上会形成单点.

可将应用服务与文件存储分开, 文件存储更是可采用分布式文件系统(建议FastDFS , Hadoop HDFS存储小文件效率比FastDFS差很多).

4.带宽

如果一块地图平均需要25个瓦片数据, 每个瓦片100KB, 如果处理时间相同情况下, 数据同时返回, 每个用户需要25 * 100KB * 8 = 20000kbps 即 20m的带宽, 这在局域网兴许可以, 但在广域网传输显然带宽占用太多,  还需要额外考虑资源压缩传输.

### 系统架构容量评估

根据业务确定评估点

- 数据量: 如银行交易流水系统
- 并发和吞吐: 如演唱会门票售卖, 双十一秒杀
- 活动带宽: 如直播平台
- 服务器CPU与GPU: AI与大数据分析系统
- 存储介质的可靠性: 如金融机构多活容灾
- 安全性与实时性: 如外贸交易系统

以最常见的商品推送为案例: 某电商网站有用户2000W,需要60分钟内完成所有的用户消息推送, 而按以往经验预估10%的用户会打开页面.

访问量: 2000W * 10% = 200W

平均QPS: 200W/ 3600 = 555 (实际上前期QPS远超于平均QPS)

>  如何评估QPS峰值?
>
>  1. 根据运营经理或数据部门推断;
>  2. 若新项目无数据支撑, 可考虑二八原则(不适用于秒杀):
>
>  ```
>  80%的流量产生在20%的时间(这也是为什么实际QPS比平均QPS高很多)
>  ```

根据二八原则, 峰值QPS为: 555 * 4 = 2220

> 测试单机QPS可通过Jmeter, LoadRunner,Postman等

假设单点设备QPS极限为400, 生产性能基线为80%(不能以QPS极限来考虑, 随时有宕机风险, 需要冗余考虑)

服务器应准备2220/(400*80%) ≈ 6.9 = 7 台

## 规约

### 为什么不使用外键

> 【阿里JAVA规范】不得使用外键与级联，一切外键概念必须在应用层解决。

1.每次做DELETE 或者UPDATE都必须考虑外键约束，会导致开发的时候很痛苦,测试数据极为不方便。

2.性能问题, 在外键关联表插入数据是会带来额外的数据一致性校验查询 .

3.并发问题,外界约束会启用行级锁 ,主表写入时外键关联表会进入阻塞.

4.级联删除问题, 多层级联删除会让数据变得不可控, 触发器也严格被禁用.

5.数据耦合, 数据库层面数据关系产生耦合, 数据迁移维护困难.

### 禁止三表join

>【强制】超过三个表禁止join . 需要join的字段 , 数据类型必须绝对一致 ; 多表关联查询时, 保证被关联的字段需要有索引。
>
>说明: 即使双表join也需要注意表索引,sql性能.

产品强制要求: 阿里OceanBase(MySql改造) 只允许两表关联 , MyCat只支持两表关联.

原因:

1.Mysql自身设计缺陷, 超过三表关联时优化器做的不好 ; 

> 银行业务无此需求 , oracle无mysql的此缺陷

2.NLJ( 嵌套循环关联 )多级嵌套性能差, 贪心与动态规划算法计算量激增;

> NLJ采用小表关联大表(小表作为驱动表,大表作为匹配表)

3.依赖数据源特性获取数据, 数据迁移改造难;

比如有商品库和订单库,一条商品记录对应对条订单记录.

当订单量巨大时, 可能需要单独迁移出一个模块分开维护, 这时候商品服务需要restful|RPC调用订单服务,  此时单条join无法完成; 

或者当订单量巨大时 需要分表存储 , 此时单条join也无法完成 ,需要使用多条in语句进行查询(在商品表中查询出所有订单表的对应信息, 然后在每个订单表分片中使用in 查询) , 而使用in只适用于数据量小(最大1000),且只支持inner join.

解决方案

反范式表:将所有关联的表拼接成一个完整的大表(数据冗余) .

在数据变更时, 将数据插入反范式表中 , 但随着数据增多, 该表同样会变得臃肿.

> 为什么不使用视图 ,视图只是逻辑表 ,查询时还是需要结合多张表查询

**最终解决方案 数据集市**

通过ETL(抽取&转换&加载)每天(T+1日终处理,第二天凌晨)对数据源进行抽取 , 在数据仓库中进行加工处理生成额外的表.

> 在ETL数据集市中,通常还会使用生成倒排表的形式关联, 比如 在对订单明细表分片时, 不根据订单明细id进行分片, 而是根据订单id进行分片, 当需要获取指定订单id的订单明细时,  不需要遍历所有分片, 只需要对订单id取模获取其对应订单明细的分片.
>
> 通过每次日终额外生成倒排表提高数据提取效率 , 典型的空间换时间.

### 禁止使用存储过程

> 【强制】禁止使用存储过程,  存储过程难以调试和扩展, 更没有移植性。

1.为什么银行都在使用存储过程?

银行业务以数据为核心 , Oracle、DB2一统江湖，存储过程与语言无关 , 预算充足，好多个W采购小型机满足性能要求 ,存储过程几乎是每一个信息科技处开发员工的入职要求. 而银行被Oracle、DB2绑架 , 数据好迁移，存储过程要全部重写  ,谁来/谁敢承担核心业务的风险？

![image-20211206203335275](picture/image-20211206203335275.png)

2.**存储过程在互联网分布式场景的问题**

① 分片场景下存储过程只能作用在局部数据(本分片);

② 数据库压力激增 

③ 无法保证分布式全局事务

![image-20211206204754565](picture/image-20211206204754565.png)

### 禁止左模糊, 搜索引擎替代

> 【强制】页面搜索严禁左模糊或者全模糊, 如果需要请走搜索引擎来解决
>
> 说明: 索引文件具有B-Tree的最左前缀匹配特性, 如果左边的值未确定 ,那么无法使用索引

左匹配 `%xxx` 将不会使用索引 `using where`

**索引选择性(关键)**

mysql中的查询优化器 **会自动根据索引和数据分布情况决定是否走索引**

右匹配也不一定会使用索引: 如在大量数据的表下对手机字段使用 `1%`模糊匹配  或在程序员表的性别字段使用精确匹配,  这就是索引选择性差带来的全表扫描.

> 在不严谨的解释下: 命中的索引值超过总量的25% ,就可能产生索引选择性陷阱, 导致全表扫描.
>
> 当然 , 一切还是以Explain执行计划为准.

可通过组合索引提高选择性 ,  如果实在不能组合 , 对于**全文检索**就只能使用ES或Solr了(ES基于分片多线程检索, 解决查询慢的问题)

但加入ES就引发了后续架构复杂度 , 数据一致性 , 分布式.

另外, 单从mysql索引选择性差的问题上的解决方案也有:

1.强制使用索引( 以需要根据优化器以实际运行效果为准),如对选择题答案过滤时

```
explain select * from question force index(answer) where answer = 'A'
```

2.增加缓存, 提高全表扫描速度(钞能力), 更可加入redis.

```
innodb_buffer_pool_size=16G
innodb_buffer_pool_instances=2
```

